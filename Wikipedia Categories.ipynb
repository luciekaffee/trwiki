{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish Wikipedia dump from: https://www.kaggle.com/mustfkeskin/turkish-wikipedia-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import codecs\n",
    "import cPickle\n",
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "from gensim import corpora\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of documents\n",
    "\n",
    "trwikidumpfile = '../turkish-wikipedia-dump'\n",
    "\n",
    "docs = {}\n",
    "\n",
    "with open(trwikidumpfile) as trwikidump:\n",
    "    data = ''\n",
    "    title = \"\"\n",
    "    for line in trwikidump:\n",
    "        if '<doc' in line and 'title=' in line:\n",
    "            title = re.sub(r'\\<doc id\\=\\\".*\\\" url\\=\\\".*\\\" title=\\\"', '', line)\n",
    "            title = title.replace('\">', '')\n",
    "        elif '</doc>' in line:\n",
    "            docs[title] = data\n",
    "            title = ''\n",
    "            data = ''\n",
    "        else:\n",
    "            data += line.decode('utf8') + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Turkish stop words\n",
    "re = requests.get('https://raw.githubusercontent.com/ahmetax/trstop/master/dosyalar/turkce-stop-words')\n",
    "stopwords = re.text\n",
    "stopwords = re.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'file' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-83683570234a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'file' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "len(docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.010*\"kız\" + 0.009*\"babası\" + 0.009*\"gün\" + 0.008*\"geri\" + 0.008*\"annesi\" + 0.008*\"son\" + 0.007*\"genç\" + 0.007*\"kızı\" + 0.007*\"oğlu\" + 0.007*\"birlikte\" \n",
      "\n",
      "2   0.012*\"akciğer\" + 0.008*\"yunan\" + 0.007*\"resim\" + 0.006*\"atina\" + 0.005*\"yunanistan\" + 0.005*\"scott\" + 0.004*\"kanser\" + 0.004*\"italya'ya\" + 0.004*\"jackson\" + 0.004*\"yunanistan'ın\" \n",
      "\n",
      "3   0.024*\"hava\" + 0.019*\"bağlı\" + 0.012*\"hüseyin\" + 0.012*\"olan,\" + 0.010*\"iran\" + 0.009*\"silahlı\" + 0.009*\"sahip,\" + 0.009*\"nüfuslu\" + 0.008*\"kuvvetleri\" + 0.008*\"cumhurbaşkanlığı\" \n",
      "\n",
      "4   0.021*\"bulunan\" + 0.017*\"han\" + 0.016*\"yüzölçümü\" + 0.016*\"yahudi\" + 0.014*\"(),\" + 0.014*\"itibarı\" + 0.012*\"tarihi\" + 0.010*\"güney\" + 0.010*\"havalimanı\" + 0.009*\"yılı\" \n",
      "\n",
      "5   0.027*\"ilk\" + 0.019*\"sezonunda\" + 0.019*\"gol\" + 0.019*\"sezon\" + 0.017*\"transfer\" + 0.016*\"maçta\" + 0.016*\"oldu.\" + 0.014*\"millî\" + 0.013*\"forma\" + 0.013*\"lig\" \n",
      "\n",
      "6   0.024*\"köy\" + 0.014*\"adı\" + 0.011*\"eski\" + 0.011*\"köyü\" + 0.011*\"göç\" + 0.010*\"ilçe\" + 0.009*\"bulunmaktadır.\" + 0.008*\"bağlı\" + 0.008*\"yerleşim\" + 0.008*\"tarım\" \n",
      "\n",
      "7   0.025*\"japon\" + 0.021*\"altın\" + 0.021*\"yaz\" + 0.019*\"madalya\" + 0.012*\"nba\" + 0.012*\"erkekler\" + 0.011*\"olimpiyat\" + 0.011*\"dünya\" + 0.011*\"gümüş\" + 0.011*\"olimpiyatları'nda\" \n",
      "\n",
      "8   0.019*\"yılında\" + 0.014*\"büyük\" + 0.009*\"üretim\" + 0.008*\"hizmet\" + 0.008*\"ilk\" + 0.008*\"sanayi\" + 0.008*\"şirket\" + 0.008*\"şirketi\" + 0.008*\"satın\" + 0.007*\"ticari\" \n",
      "\n",
      "9   0.027*\"yarı\" + 0.013*\"elektron\" + 0.009*\"tank\" + 0.009*\"eurovision\" + 0.009*\"adet\" + 0.009*\"ulusal\" + 0.008*\"top\" + 0.008*\"fotoğraf\" + 0.007*\"finalde\" + 0.007*\"balık\" \n",
      "\n",
      "10   0.082*\"align=\"center\"\" + 0.023*\"futbol\" + 0.023*\"colspan=\"4\"\" + 0.017*\"toplam\" + 0.013*\"ligi\" + 0.012*\"türkiye\" + 0.012*\"sayısı\" + 0.011*\"sonuçlar\" + 0.011*\"fifa\" + 0.010*\"uefa\" \n",
      "\n",
      "11   0.046*\"yılında\" + 0.023*\"arasında\" + 0.020*\"olmuştur.\" + 0.012*\"yılları\" + 0.012*\"yıllarında\" + 0.007*\"döneminde\" + 0.007*\"almıştır.\" + 0.007*\"anadolu\" + 0.005*\"bulunmuştur.\" + 0.005*\"etmiştir.\" \n",
      "\n",
      "12   0.066*\"the\" + 0.025*\"and\" + 0.010*\"for\" + 0.009*\"david\" + 0.009*\"listesi\" + 0.007*\"los\" + 0.007*\"michael\" + 0.006*\"city\" + 0.006*\"black\" + 0.005*\"\"the\" \n",
      "\n",
      "13   0.022*\"bilim\" + 0.012*\"üzerine\" + 0.009*\"bilimsel\" + 0.007*\"alanında\" + 0.007*\"fizik\" + 0.006*\"felsefe\" + 0.006*\"von\" + 0.005*\"hasan\" + 0.005*\"çalışmalar\" + 0.004*\"müdürlüğü,\" \n",
      "\n",
      "14   0.009*\"büyük\" + 0.008*\"ortaya\" + 0.008*\"zaman\" + 0.007*\"gelen\" + 0.006*\"aynı\" + 0.006*\"sadece\" + 0.006*\"önemli\" + 0.006*\"özellikle\" + 0.005*\"başka\" + 0.005*\"farklı\" \n",
      "\n",
      "15   0.033*\"yılında\" + 0.032*\"tarihinde\" + 0.015*\"ilk\" + 0.011*\"mayıs\" + 0.011*\"olmuştur.\" + 0.010*\"2012\" + 0.009*\"dünya\" + 0.009*\"2014\" + 0.009*\"2013\" + 0.009*\"2008\" \n",
      "\n",
      "16   0.018*\"enerji\" + 0.012*\"yüksek\" + 0.010*\"elektrik\" + 0.009*\"güneş\" + 0.009*\"nükleer\" + 0.008*\"kimyasal\" + 0.007*\"madde\" + 0.007*\"kullanılır.\" + 0.007*\"hücre\" + 0.006*\"meydana\" \n",
      "\n",
      "17   0.011*\"küçük\" + 0.010*\"kara\" + 0.010*\"beyaz\" + 0.009*\"deniz\" + 0.009*\"güney\" + 0.008*\"bulunur.\" + 0.008*\"alt\" + 0.007*\"türü\" + 0.007*\"bitki\" + 0.007*\"kuzey\" \n",
      "\n",
      "18   0.045*\"film\" + 0.027*\"filmin\" + 0.023*\"yapımı\" + 0.022*\"sinema\" + 0.017*\"rol\" + 0.016*\"tiyatro\" + 0.016*\"filmi\" + 0.015*\"televizyon\" + 0.015*\"filmde\" + 0.014*\"oyuncu\" \n",
      "\n",
      "19   0.061*\"(d.\" + 0.037*\"futbolcudur.\" + 0.029*\"yılında\" + 0.029*\"profesyonel\" + 0.025*\"görev\" + 0.024*\"başladı.\" + 0.023*\"eski\" + 0.021*\"millî\" + 0.020*\"kariyerine\" + 0.016*\"oynadı.\" \n",
      "\n",
      "20   0.008*\"inşa\" + 0.008*\"ilk\" + 0.007*\"adet\" + 0.006*\"istanbul\" + 0.006*\"türkiye'nin\" + 0.006*\"bulunmaktadır.\" + 0.005*\"önemli\" + 0.005*\"yıllarda\" + 0.004*\"bulunan\" + 0.004*\"edilen\" \n",
      "\n",
      "21   0.029*\"osmanlı\" + 0.014*\"ii.\" + 0.012*\"bizans\" + 0.012*\"oğlu\" + 0.010*\"büyük\" + 0.009*\"üzerine\" + 0.008*\"islam\" + 0.007*\"muhammed\" + 0.007*\"tahta\" + 0.007*\"idi.\" \n",
      "\n",
      "22   0.021*\"genel\" + 0.021*\"türk\" + 0.017*\"görev\" + 0.015*\"(d.\" + 0.014*\"devlet\" + 0.014*\"ankara\" + 0.013*\"dönem\" + 0.013*\"istanbul\" + 0.012*\"yapmıştır.\" + 0.012*\"arasında\" \n",
      "\n",
      "23   0.022*\"genel\" + 0.011*\"seçimleri\" + 0.011*\"türkiye\" + 0.008*\"motor\" + 0.008*\"sonuçlandı.\" + 0.007*\"wwe\" + 0.007*\"euro\" + 0.006*\"zamanlı\" + 0.005*\"anahtar\" + 0.005*\"haftada\" \n",
      "\n",
      "24   0.030*\"müzik\" + 0.021*\"şarkı\" + 0.021*\"ilk\" + 0.015*\"albümü\" + 0.014*\"albüm\" + 0.013*\"grup\" + 0.012*\"adlı\" + 0.011*\"yer\" + 0.010*\"grubun\" + 0.010*\"albümün\" \n",
      "\n",
      "25   0.009*\"israil\" + 0.006*\"atom\" + 0.006*\"optik\" + 0.005*\"hakkında\" + 0.004*\"cup\" + 0.004*\"yük\" + 0.004*\"sonuç\" + 0.004*\"iddia\" + 0.004*\"parçacık\" + 0.004*\"dini\" \n",
      "\n",
      "26   0.044*\"yılında\" + 0.025*\"oldu.\" + 0.020*\"aldı.\" + 0.015*\"başladı.\" + 0.015*\"yaptı.\" + 0.014*\"(d.\" + 0.014*\"adlı\" + 0.013*\"etti.\" + 0.012*\"ilk\" + 0.011*\"yıl\" \n",
      "\n",
      "27   0.024*\"ilk\" + 0.019*\"dünya\" + 0.017*\"ikinci\" + 0.015*\"kazandı.\" + 0.014*\"oldu.\" + 0.013*\"takım\" + 0.010*\"son\" + 0.010*\"grand\" + 0.009*\"finalde\" + 0.009*\"maçı\" \n",
      "\n",
      "28   0.018*\"anlamlara\" + 0.015*\"gelebilir:\" + 0.013*\"del\" + 0.011*\"san\" + 0.011*\"(anlam\" + 0.011*\"ayrımı)\" + 0.009*\"aşağıdaki\" + 0.006*\"topu\" + 0.006*\"kızılderili\" + 0.005*\"antonio\" \n",
      "\n",
      "29   0.015*\"kadın\" + 0.006*\"toplumsal\" + 0.006*\"roman\" + 0.005*\"kadınların\" + 0.005*\"sosyal\" + 0.005*\"kitabın\" + 0.004*\"kadınlar\" + 0.004*\"düşünce\" + 0.004*\"insan\" + 0.004*\"insanların\" \n",
      "\n",
      "30   0.032*\"iyi\" + 0.017*\"\"the\" + 0.016*\"film\" + 0.015*\"ödülü\" + 0.012*\"yer\" + 0.010*\"john\" + 0.010*\"stüdyo\" + 0.008*\"amerikan\" + 0.008*\"ödül\" + 0.008*\"listesinde\" \n",
      "\n",
      "31   0.019*\"siyah\" + 0.014*\"kırmızı\" + 0.013*\"mavi\" + 0.013*\"renkli\" + 0.011*\"sarı\" + 0.011*\"koyu\" + 0.010*\"yeşil\" + 0.009*\"familyasından\" + 0.009*\"renk\" + 0.008*\"lee\" \n",
      "\n",
      "32   0.073*\"birleşik\" + 0.051*\"amerika\" + 0.028*\"abd\" + 0.018*\"amerikan\" + 0.014*\"ilçenin\" + 0.012*\"merkezi\" + 0.011*\"krallık\" + 0.011*\"itibarıyla\" + 0.010*\"abd'nin\" + 0.010*\"yılı\" \n",
      "\n",
      "33   0.015*\"savaş\" + 0.013*\"karşı\" + 0.012*\"savaşı\" + 0.010*\"askeri\" + 0.009*\"alman\" + 0.009*\"sırasında\" + 0.009*\"yılında\" + 0.009*\"ordusu\" + 0.008*\"ii.\" + 0.008*\"ele\" \n",
      "\n",
      "34   0.012*\"oyun\" + 0.012*\"yeni\" + 0.012*\"windows\" + 0.010*\"ilk\" + 0.008*\"web\" + 0.008*\"bilgisayar\" + 0.007*\"oyunun\" + 0.006*\"video\" + 0.006*\"veri\" + 0.006*\"piyasaya\" \n",
      "\n",
      "35   0.016*\"ilk\" + 0.016*\"adlı\" + 0.007*\"aynı\" + 0.007*\"yazar\" + 0.006*\"hakkında\" + 0.006*\"kitap\" + 0.006*\"yazdığı\" + 0.005*\"kitabı\" + 0.004*\"eseri\" + 0.004*\"konu\" \n",
      "\n",
      "36   0.034*\"yoktur.\" + 0.031*\"köyün\" + 0.028*\"vardır.\" + 0.028*\"ilçesine\" + 0.025*\"sağlık\" + 0.024*\"şebekesi\" + 0.023*\"ptt\" + 0.022*\"mahallenin\" + 0.016*\"bağlı\" + 0.015*\"ilinin\" \n",
      "\n",
      "37   0.029*\"yer\" + 0.021*\"bulunan\" + 0.016*\"büyük\" + 0.013*\"şehir\" + 0.012*\"alan\" + 0.009*\"bağlı\" + 0.009*\"bulunmaktadır.\" + 0.009*\"kuzey\" + 0.009*\"merkezi\" + 0.009*\"almaktadır.\" \n",
      "\n",
      "38   0.018*\"avrupa\" + 0.015*\"doğu\" + 0.014*\"batı\" + 0.012*\"arasında\" + 0.009*\"barış\" + 0.009*\"ülke\" + 0.008*\"büyük\" + 0.008*\"fransa\" + 0.007*\"hollanda\" + 0.007*\"almanya\" \n",
      "\n",
      "39   0.011*\"mısır\" + 0.010*\"sivas\" + 0.009*\"bulunan\" + 0.008*\"suriye\" + 0.007*\"selçuklu\" + 0.007*\"gayet\" + 0.007*\"hacı\" + 0.006*\"şeyh\" + 0.006*\"ağa\" + 0.006*\"saray\" \n",
      "\n",
      "40   0.021*\"new\" + 0.014*\"york\" + 0.011*\"söyler.\" + 0.008*\"eder.\" + 0.007*\"cinsel\" + 0.007*\"oyunculuk\" + 0.006*\"eve\" + 0.005*\"seyahat\" + 0.005*\"canlandırdığı\" + 0.005*\"vize\" \n",
      "\n",
      "41   0.008*\"internet\" + 0.007*\"bilgi\" + 0.007*\"temel\" + 0.006*\"sistemi\" + 0.005*\"açık\" + 0.005*\"yeni\" + 0.005*\"sistem\" + 0.005*\"para\" + 0.004*\"kontrol\" + 0.004*\"özel\" \n",
      "\n",
      "42   0.014*\"uzay\" + 0.008*\"güneş\" + 0.008*\"dalga\" + 0.007*\"aynı\" + 0.007*\"zaman\" + 0.006*\"ifade\" + 0.006*\"yıldız\" + 0.006*\"alan\" + 0.005*\"temel\" + 0.005*\"manyetik\" \n",
      "\n",
      "43   0.014*\"ilk\" + 0.011*\"büyük\" + 0.009*\"yılında\" + 0.008*\"fransız\" + 0.008*\"yeni\" + 0.006*\"yıl\" + 0.006*\"dünya\" + 0.005*\"arasında\" + 0.005*\"ingiliz\" + 0.005*\"önemli\" \n",
      "\n",
      "44   0.029*\"yılında\" + 0.024*\"eğitim\" + 0.023*\"üniversitesi\" + 0.013*\"uluslararası\" + 0.012*\"türk\" + 0.010*\"yüksek\" + 0.009*\"istanbul\" + 0.008*\"öğretim\" + 0.008*\"kültür\" + 0.007*\"türkiye\" \n",
      "\n",
      "45   0.021*\"çin\" + 0.012*\"kan\" + 0.007*\"tıbbi\" + 0.007*\"sinir\" + 0.006*\"çin'in\" + 0.006*\"kalın\" + 0.005*\"insan\" + 0.005*\"hindistan\" + 0.005*\"hint\" + 0.005*\"besin\" \n",
      "\n",
      "46   0.052*\"şehrin\" + 0.045*\"yer\" + 0.042*\"alan\" + 0.040*\"nüfus\" + 0.036*\"nüfusu\" + 0.033*\"tarihinde\" + 0.029*\"edilmiştir.\" + 0.027*\"tespit\" + 0.025*\"amerikan,\" + 0.025*\"şehirdir.\" \n",
      "\n",
      "47   0.046*\"türk\" + 0.033*\"ali\" + 0.027*\"paşa\" + 0.023*\"mustafa\" + 0.020*\"mehmet\" + 0.016*\"ahmet\" + 0.014*\"bey\" + 0.013*\"sultan\" + 0.013*\"kemal\" + 0.012*\"ibrahim\" \n",
      "\n",
      "48   0.009*\"yüksek\" + 0.008*\"kullanılan\" + 0.007*\"genellikle\" + 0.007*\"farklı\" + 0.006*\"sahip\" + 0.006*\"yaklaşık\" + 0.006*\"küçük\" + 0.006*\"ışık\" + 0.005*\"bulunan\" + 0.005*\"aynı\" \n",
      "\n",
      "49   0.013*\"yılı\" + 0.013*\"kişi\" + 0.011*\"2006\" + 0.010*\"resmi\" + 0.010*\"dil\" + 0.009*\"yönetim\" + 0.008*\"iran'ın\" + 0.008*\"bulgar\" + 0.008*\"merkezi\" + 0.008*\"posta\" \n",
      "\n",
      "50   0.008*\"karşı\" + 0.005*\"yeni\" + 0.005*\"devlet\" + 0.005*\"birliği\" + 0.005*\"siyasi\" + 0.005*\"sovyet\" + 0.004*\"etti.\" + 0.004*\"kabul\" + 0.004*\"ardından\" + 0.004*\"sovyetler\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic modelling based on \n",
    "# https://github.com/abhijeet3922/Topic-Modelling-on-Wiki-corpus/blob/master/wiki_topic_model.py\n",
    "\n",
    "# Function to remove stop words from sentences & lemmatize verbs. \n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords])\n",
    "    normalized = \" \".join(lemma.lemmatize(word,'v') for word in stop_free.split())\n",
    "    x = normalized.split()\n",
    "    y = [s for s in x if len(s) > 2]\n",
    "    return y\n",
    "\n",
    "doc_complete = docs.values()\n",
    "\n",
    "# Randomly sample 70000 articles from the corpus     \n",
    "docs_all = random.sample(doc_complete, 70000)\n",
    "#docs = open(\"docs_wiki.pkl\",'wb')\n",
    "#cPickle.dump(docs_all,docs)\n",
    "\n",
    "# Use 60000 articles for training.\n",
    "docs_train = docs_all[:60000]\n",
    "\n",
    "\n",
    "# Cleaning all the 60,000 simplewiki articles\n",
    "lemma = WordNetLemmatizer()\n",
    "doc_clean = [clean(doc) for doc in docs_train]\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Filter the terms which have occured in less than 3 articles and more than 40% of the articles \n",
    "dictionary.filter_extremes(no_below=4, no_above=0.4)\n",
    "\n",
    "#words,ids = dictionary.filter_n_most_frequent(50)\n",
    "#print words,\"\\n\\n\",ids\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#Creating the object for LDA model using gensim library & Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=50, id2word = dictionary, passes=50, iterations=500)\n",
    "ldafile = open('lda_model_sym_wiki.pkl','wb')\n",
    "cPickle.dump(ldamodel,ldafile)\n",
    "ldafile.close()\n",
    "\n",
    "#Print all the 50 topics\n",
    "for topic in ldamodel.print_topics(num_topics=50, num_words=10):\n",
    "    print topic[0]+1, \" \", topic[1],\"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
